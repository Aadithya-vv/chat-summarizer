# Chat Summarizer ğŸ§ ğŸ’¬

A web application that summarizes long chat conversations (100+ unread messages) into clear, structured insights using a **local Large Language Model (LLM)**.

---

## ğŸš€ Features

- Paste long chat conversations (WhatsApp / Discord / Slack style)
- AI-generated structured summary:
  - ğŸ§  Main Topics
  - âœ… Decisions
  - ğŸ›  Action Items
- Optimized for large chats using chunking
- Runs **fully locally** (no paid APIs)
- ChatGPT-style dark UI
- Export summary:
  - ğŸ“‹ Copy to clipboard
  - ğŸ“„ Download as `.txt`

---

## ğŸ§  How It Works (High Level)

1. User pastes a long chat conversation
2. Backend preprocesses and cleans the text
3. Chat is summarized using a **local LLM (Mistral via Ollama)**
4. Output is structured into topics, decisions, and actions
5. Frontend renders a clean, readable summary

---

## ğŸ—ï¸ Architecture

Frontend (React)
|
| POST /summarize
|
Backend (FastAPI)
|
| Prompt + Chunking
|
Local LLM (Ollama - Mistral 7B)


---

## ğŸ› ï¸ Tech Stack

### Frontend
- React (Vite)
- Plain CSS / Inline styles
- Fetch API

### Backend
- Python
- FastAPI
- Requests

### AI / ML
- Ollama
- Mistral 7B (local inference)

---

## âš¡ Performance Optimizations

- Chunking for very large chats
- Fast-path summarization for smaller chats
- Token limits and low-temperature decoding
- Reduced LLM calls for lower latency

Typical summary time (local):
- 100â€“150 messages â†’ ~10â€“15 seconds

---

## â–¶ï¸ Running the Project Locally

### 1ï¸âƒ£ Start the local LLM
```bash
ollama pull mistral
ollama serve
